{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def _process_csv_file(file):\n",
    "    with open(file, 'r') as fr:\n",
    "        files = fr.readlines()\n",
    "    return files\n",
    "\n",
    "class BalanceCovidDataset(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_dir,\n",
    "            csv_file,\n",
    "            is_training=True,\n",
    "            batch_size=8,\n",
    "            input_shape=(224, 224),\n",
    "            n_classes=3,\n",
    "            num_channels=3,\n",
    "            mapping={\n",
    "                'normal': 0,\n",
    "                'pneumonia': 1,\n",
    "                'COVID-19': 2\n",
    "            },\n",
    "            shuffle=True,\n",
    "            augmentation=True,\n",
    "            covid_percent=0.3,\n",
    "            class_weights=[1., 1., 6.]\n",
    "    ):\n",
    "        'Initialization'\n",
    "        self.datadir = data_dir\n",
    "        self.dataset = _process_csv_file(csv_file)\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = batch_size\n",
    "        self.N = len(self.dataset)\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.num_channels = num_channels\n",
    "        self.mapping = mapping\n",
    "        self.shuffle = True\n",
    "        self.covid_percent = covid_percent\n",
    "        self.class_weights = class_weights\n",
    "        self.n = 0\n",
    "\n",
    "        if augmentation:\n",
    "            self.augmentation = ImageDataGenerator(\n",
    "                featurewise_center=False,\n",
    "                featurewise_std_normalization=False,\n",
    "                rotation_range=10,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                brightness_range=(0.9, 1.1),\n",
    "                zoom_range=(0.85, 1.15),\n",
    "                fill_mode='constant',\n",
    "                cval=0.,\n",
    "            )\n",
    "\n",
    "        datasets = {'normal': [], 'pneumonia': [], 'COVID-19': []}\n",
    "        for l in self.dataset:\n",
    "            datasets[l.split()[-1]].append(l)\n",
    "        self.datasets = [\n",
    "            datasets['normal'] + datasets['pneumonia'],\n",
    "            datasets['COVID-19'],\n",
    "        ]\n",
    "        print(len(self.datasets[0]), len(self.datasets[1]))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        batch_x, batch_y, weights = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "\n",
    "        return batch_x, batch_y, weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            for v in self.datasets:\n",
    "                np.random.shuffle(v)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x, batch_y = np.zeros(\n",
    "            (self.batch_size, *self.input_shape,\n",
    "             self.num_channels)), np.zeros(self.batch_size)\n",
    "\n",
    "        batch_files = self.datasets[0][idx * self.batch_size:(idx + 1) *\n",
    "                                       self.batch_size]\n",
    "\n",
    "        # upsample covid cases\n",
    "        covid_size = max(int(len(batch_files) * self.covid_percent), 1)\n",
    "        covid_inds = np.random.choice(np.arange(len(batch_files)),\n",
    "                                      size=covid_size,\n",
    "                                      replace=False)\n",
    "        covid_files = np.random.choice(self.datasets[1],\n",
    "                                       size=covid_size,\n",
    "                                       replace=False)\n",
    "        for i in range(covid_size):\n",
    "            batch_files[covid_inds[i]] = covid_files[i]\n",
    "\n",
    "        for i in range(len(batch_files)):\n",
    "            sample = batch_files[i].split()\n",
    "\n",
    "            if self.is_training:\n",
    "                folder = 'train'\n",
    "            else:\n",
    "                folder = 'test'\n",
    "\n",
    "            x = cv2.imread(os.path.join(self.datadir, folder, sample[1]))\n",
    "            h, w, c = x.shape\n",
    "            x = x[int(h/6):, :]\n",
    "            x = cv2.resize(x, self.input_shape)\n",
    "\n",
    "            if self.is_training and hasattr(self, 'augmentation'):\n",
    "                x = self.augmentation.random_transform(x)\n",
    "\n",
    "            x = x.astype('float32') / 255.0\n",
    "            y = self.mapping[sample[2]]\n",
    "\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "\n",
    "        class_weights = self.class_weights\n",
    "        weights = np.take(class_weights, batch_y.astype('int64'))\n",
    "\n",
    "        return batch_x, keras.utils.to_categorical(batch_y, num_classes=self.n_classes), weights\n",
    "\n",
    "class BalanceDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 is_training=True,\n",
    "                 batch_size=8,\n",
    "                 input_shape=(224,224),\n",
    "                 n_classes=3,\n",
    "                 num_channels=3,\n",
    "                 mapping={'normal': 0, 'pneumonia': 1, 'COVID-19': 2},\n",
    "                 shuffle=True,\n",
    "                 augmentation=True,\n",
    "                 datadir='data',\n",
    "                 class_weights=[1., 1., 25.]\n",
    "                 ):\n",
    "        'Initialization'\n",
    "        self.datadir = datadir\n",
    "        self.dataset = dataset\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = batch_size\n",
    "        self.N = len(self.dataset)\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.num_channels = num_channels\n",
    "        self.mapping = mapping\n",
    "        self.shuffle = True\n",
    "        self.n = 0\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        if augmentation:\n",
    "            self.augmentation = ImageDataGenerator(\n",
    "                featurewise_center=False,\n",
    "                featurewise_std_normalization=False,\n",
    "                rotation_range=10,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                brightness_range=(0.9, 1.1),\n",
    "                fill_mode='constant',\n",
    "                cval=0.,\n",
    "            )\n",
    "\n",
    "        datasets = {'normal': [], 'pneumonia': [], 'COVID-19': []}\n",
    "        for l in dataset:\n",
    "            datasets[l.split()[-1]].append(l)\n",
    "        self.datasets = [\n",
    "            datasets['normal'] + datasets['pneumonia'],\n",
    "            datasets['COVID-19'],\n",
    "        ]\n",
    "        print(len(self.datasets[0]), len(self.datasets[1]))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        batch_x, batch_y, weights = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "\n",
    "        return batch_x, batch_y, weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            for v in self.datasets:\n",
    "                np.random.shuffle(v)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x, batch_y = np.zeros((self.batch_size, *self.input_shape, self.num_channels)), np.zeros(self.batch_size)\n",
    "\n",
    "        batch_files = self.datasets[0][idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        batch_files[np.random.randint(self.batch_size)] = np.random.choice(self.datasets[1])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            sample = batch_files[i].split()\n",
    "\n",
    "            if self.is_training:\n",
    "                folder = 'train'\n",
    "            else:\n",
    "                folder = 'test'\n",
    "\n",
    "            x = cv2.imread(os.path.join(self.datadir, folder, sample[1]))\n",
    "            h, w, c = x.shape\n",
    "            x = x[int(h/6):, :]\n",
    "            x = cv2.resize(x, self.input_shape)\n",
    "\n",
    "            if self.is_training and hasattr(self, 'augmentation'):\n",
    "                x = self.augmentation.random_transform(x)\n",
    "\n",
    "            x = x.astype('float32') / 255.0\n",
    "            y = self.mapping[sample[2]]\n",
    "\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "\n",
    "        weights = np.take(self.class_weights, batch_y.astype('int64'))\n",
    "\n",
    "        return batch_x, keras.utils.to_categorical(batch_y, num_classes=self.n_classes), weights\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 is_training=True,\n",
    "                 batch_size=8,\n",
    "                 input_shape=(224,224),\n",
    "                 n_classes=3,\n",
    "                 num_channels=3,\n",
    "                 mapping={'normal': 0, 'pneumonia': 1, 'COVID-19': 2},\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = batch_size\n",
    "        self.N = len(self.dataset)\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.num_channels = num_channels\n",
    "        self.mapping = mapping\n",
    "        self.shuffle = True\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.N / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.dataset = shuffle(self.dataset, random_state=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x, batch_y = np.zeros((self.batch_size, *self.input_shape, self.num_channels)), np.zeros(self.batch_size)\n",
    "        for i in range(self.batch_size):\n",
    "            index = min((idx * self.batch_size) + i, self.N-1)\n",
    "\n",
    "            sample = self.dataset[index].split()\n",
    "\n",
    "            if self.is_training:\n",
    "                folder = 'train'\n",
    "            else:\n",
    "                folder = 'test'\n",
    "\n",
    "            x = cv2.imread(os.path.join('data', folder, sample[1]))\n",
    "            x = cv2.resize(x, self.input_shape)\n",
    "            x = x.astype('float32') / 255.0\n",
    "            #y = int(sample[1])\n",
    "            y = self.mapping[sample[2]]\n",
    "\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "\n",
    "        return batch_x, keras.utils.to_categorical(batch_y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, argparse\n",
    "import cv2\n",
    "\n",
    "mapping = {'normal': 0, 'pneumonia': 1, 'COVID-19': 2}\n",
    "\n",
    "def eval(sess, graph, testfile, testfolder):\n",
    "    image_tensor = graph.get_tensor_by_name(\"input_1:0\")\n",
    "    pred_tensor = graph.get_tensor_by_name(\"dense_3/Softmax:0\")\n",
    "\n",
    "    y_test = []\n",
    "    pred = []\n",
    "    for i in range(len(testfile)):\n",
    "        line = testfile[i].split()\n",
    "        x = cv2.imread(os.path.join('data', testfolder, line[1]))\n",
    "        h, w, c = x.shape\n",
    "        x = x[int(h/6):, :]\n",
    "        x = cv2.resize(x, (224, 224))\n",
    "        x = x.astype('float32') / 255.0\n",
    "        y_test.append(mapping[line[2]])\n",
    "        pred.append(np.array(sess.run(pred_tensor, feed_dict={image_tensor: np.expand_dims(x, axis=0)})).argmax(axis=1))\n",
    "    y_test = np.array(y_test)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    matrix = confusion_matrix(y_test, pred)\n",
    "    matrix = matrix.astype('float')\n",
    "    #cm_norm = matrix / matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(matrix)\n",
    "    #class_acc = np.array(cm_norm.diagonal())\n",
    "    class_acc = [matrix[i,i]/np.sum(matrix[i,:]) if np.sum(matrix[i,:]) else 0 for i in range(len(matrix))]\n",
    "    print('Sens Normal: {0:.3f}, Pneumonia: {1:.3f}, COVID-19: {2:.3f}'.format(class_acc[0],\n",
    "                                                                               class_acc[1],\n",
    "                                                                               class_acc[2]))\n",
    "    ppvs = [matrix[i,i]/np.sum(matrix[:,i]) if np.sum(matrix[:,i]) else 0 for i in range(len(matrix))]\n",
    "    print('PPV Normal: {0:.3f}, Pneumonia {1:.3f}, COVID-19: {2:.3f}'.format(ppvs[0],\n",
    "                                                                             ppvs[1],\n",
    "                                                                             ppvs[2]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    weightspath=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master'\n",
    "    metaname='model.meta'\n",
    "    ckptname='model'\n",
    "    testfile=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\test_COVIDx.txt'\n",
    "    testfolder=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\data\\test'\n",
    "\n",
    "    \n",
    "\n",
    "    '''sess = tf.Session()\n",
    "    tf.get_default_graph()\n",
    "    saver = tf.train.import_meta_graph(os.path.join(weightspath, metaname))\n",
    "    saver.restore(sess, os.path.join(weightspath, ackptname))\n",
    "\n",
    "    graph = tf.get_default_graph()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 31\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='COVID-Net Training Script')\n",
    "epochs=10\n",
    "lr=0.00002\n",
    "bs=8\n",
    "weightspath=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\models\\COVIDNet-CXR-Large'\n",
    "metaname='model.meta'\n",
    "ckptname='model-8485'\n",
    "trainfile=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\train_COVIDx2.txt'\n",
    "testfile=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\test_COVIDx2.txt'\n",
    "name=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\COVIDNet'\n",
    "datadir=r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\data'\n",
    "covid_weight=12.\n",
    "covid_percent=0.3\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate =lr\n",
    "batch_size = bs\n",
    "display_step = 1\n",
    "\n",
    "'''# output path\n",
    "outputPath = './output/'\n",
    "runID = name + '-lr' + str(learning_rate)\n",
    "runPath = outputPath + runID\n",
    "pathlib.Path(runPath).mkdir(parents=True, exist_ok=True)\n",
    "print('Output: ' + runPath)'''\n",
    "\n",
    "with open(trainfile) as f:\n",
    "    trainfiles = f.readlines()\n",
    "with open(testfile) as f:\n",
    "    testfiles = f.readlines()\n",
    "\n",
    "generator = BalanceCovidDataset(data_dir=datadir,\n",
    "                                csv_file=testfile,\n",
    "                                covid_percent=covid_percent,\n",
    "                                is_training=False,\n",
    "                                class_weights=[1., 1.,covid_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, y_valid, weights = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "        batch_x, batch_y, weights = next(generator)\n",
    "        x_valid=np.vstack((x_valid,batch_x))\n",
    "        y_valid=np.vstack((y_valid,batch_y))\n",
    "        weights=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\numpy\\x_valid',x_valid)\n",
    "np.save(r'C:\\Users\\siris\\OneDrive\\Desktop\\215\\Final_Project\\data\\COVID-Net-master\\numpy\\y_valid',y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
